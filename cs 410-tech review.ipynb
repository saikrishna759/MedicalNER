{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1QjXN5IHXagzq54k2e-kYfwNAV8x3WhpM","authorship_tag":"ABX9TyMS+Mvg4kKcl/cwC1B4/P4f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ynyjLidZPGqi"},"outputs":[],"source":["import numpy as np\n","from keras.models import Model, load_model\n","from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n","    Flatten, concatenate\n","from preprocess import readfile, createBatches, createMatrices, iterate_minibatches, addCharInformation, padding, compute_f1\n","from keras.utils import plot_model\n","from keras.initializers import RandomUniform\n","from keras.optimizers import SGD, Nadam"]},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n","\n","model_path = '/content/drive/MyDrive/cs410_glove/GoogleNews-vectors-negative300.bin'\n","fEmbeddings = KeyedVectors.load_word2vec_format(model_path, binary=True)"],"metadata":{"id":"Xbj2FQhOG0oE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fEmbeddings.index_to_key[10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"R01X1zVIG2Cr","executionInfo":{"status":"ok","timestamp":1701576250549,"user_tz":360,"elapsed":27,"user":{"displayName":"Saikrishna Sanniboina","userId":"04267939723008232652"}},"outputId":"5fc1a75a-c8ff-43cc-eb1c-8d874af2951e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'was'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\"\"\"Initialise class\"\"\"\n","from gensim.models import KeyedVectors\n","\n","class CNN_BLSTM(object):\n","\n","    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n","\n","        self.epochs = EPOCHS\n","        self.dropout = DROPOUT\n","        self.dropout_recurrent = DROPOUT_RECURRENT\n","        self.lstm_state_size = LSTM_STATE_SIZE\n","        self.conv_size = CONV_SIZE\n","        self.learning_rate = LEARNING_RATE\n","        self.optimizer = OPTIMIZER\n","\n","    def loadData(self):\n","        \"\"\"Load data and add character information\"\"\"\n","        self.trainSentences = readfile(\"/content/drive/MyDrive/cs410_data/train.txt\")\n","        self.devSentences = readfile(\"/content/drive/MyDrive/cs410_data/dev.txt\")\n","        self.testSentences = readfile(\"/content/drive/MyDrive/cs410_data/test.txt\")\n","\n","    def addCharInfo(self):\n","        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n","        self.trainSentences = addCharInformation(self.trainSentences)\n","        self.devSentences = addCharInformation(self.devSentences)\n","        self.testSentences = addCharInformation(self.testSentences)\n","\n","    def embed(self):\n","        \"\"\"Create word- and character-level embeddings\"\"\"\n","\n","        labelSet = set()\n","        words = {}\n","\n","        # unique words and labels in data\n","        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n","            for sentence in dataset:\n","                for token, char, label in sentence:\n","                    # token ... token, char ... list of chars, label ... BIO labels\n","                    labelSet.add(label)\n","                    words[token.lower()] = True\n","\n","        # mapping for labels\n","        self.label2Idx = {}\n","        for label in labelSet:\n","            self.label2Idx[label] = len(self.label2Idx)\n","\n","        # mapping for token cases\n","        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n","                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n","        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used\n","\n","        # read GLoVE word embeddings\n","        word2Idx = {}\n","        self.wordEmbeddings = []\n","\n","        # fEmbeddings = open(\"/content/drive/MyDrive/cs410_glove/glove.6B.50d.txt\", encoding=\"utf-8\")\n","\n","        model_path = '/content/drive/MyDrive/cs410_glove/GoogleNews-vectors-negative300.bin'\n","\n","        fEmbeddings = KeyedVectors.load_word2vec_format(model_path, binary=True)\n","        # loop through each word in embeddings\n","        for ind in range(len(fEmbeddings)):\n","            word = fEmbeddings.index_to_key[ind]\n","            # split = line.strip().split(\" \")\n","            vector_emb = fEmbeddings[word]\n","\n","            if len(word2Idx) == 0:  # add padding+unknown\n","                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n","                vector = np.zeros(len(vector_emb))  # zero vector for 'PADDING' word\n","                self.wordEmbeddings.append(vector)\n","\n","                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n","                vector = np.random.uniform(-0.25, 0.25, len(vector_emb))\n","                self.wordEmbeddings.append(vector)\n","\n","            if word in words:\n","                # vector = np.array([float(num) for num in split[1:]])\n","                self.wordEmbeddings.append(vector_emb)  # word embedding vector\n","                word2Idx[word] = len(word2Idx)  # corresponding word dict\n","\n","        self.wordEmbeddings = np.array(self.wordEmbeddings)\n","\n","        # dictionary of all possible characters\n","        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n","        for c in \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n","\n","            self.char2Idx[c] = len(self.char2Idx)\n","\n","        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n","        self.train_set = padding(createMatrices(self.trainSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n","        self.dev_set = padding(createMatrices(self.devSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n","        self.test_set = padding(createMatrices(self.testSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n","\n","        self.idx2Label = {v: k for k, v in self.label2Idx.items()}\n","\n","    def createBatches(self):\n","        \"\"\"Create batches\"\"\"\n","        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n","        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n","        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n","\n","    def tag_dataset(self, dataset, model):\n","        \"\"\"Tag data with numerical values\"\"\"\n","        correctLabels = []\n","        predLabels = []\n","        for i, data in enumerate(dataset):\n","            tokens, casing, char, labels = data\n","            tokens = np.asarray([tokens])\n","            casing = np.asarray([casing])\n","            char = np.asarray([char])\n","            pred = model.predict([tokens, casing, char], verbose=False)[0]\n","            pred = pred.argmax(axis=-1)  # Predict the classes\n","            correctLabels.append(labels)\n","            predLabels.append(pred)\n","        return predLabels, correctLabels\n","\n","    def buildModel(self):\n","        \"\"\"Model layers\"\"\"\n","\n","        # character input\n","        character_input = Input(shape=(None, 52,), name=\"Character_input\")\n","        embed_char_out = TimeDistributed(\n","            Embedding(len(self.char2Idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n","            character_input)\n","\n","        dropout = Dropout(self.dropout)(embed_char_out)\n","\n","        # CNN\n","        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n","        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)\n","        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n","        char = Dropout(self.dropout)(char)\n","\n","        # word-level input\n","        words_input = Input(shape=(None,), dtype='int32', name='words_input')\n","        words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self.wordEmbeddings],\n","                          trainable=False)(words_input)\n","\n","        # case-info input\n","        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n","        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n","                           trainable=False)(casing_input)\n","\n","        # concat & BLSTM\n","        output = concatenate([words, casing, char])\n","        output = Bidirectional(LSTM(self.lstm_state_size,\n","                                    return_sequences=True,\n","                                    dropout=self.dropout,                        # on input to each LSTM block\n","                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n","                                   ), name=\"BLSTM\")(output)\n","        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n","\n","        # set up model\n","        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n","\n","        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n","\n","        self.init_weights = self.model.get_weights()\n","\n","        plot_model(self.model, to_file='model.png')\n","\n","        print(\"Model built. Saved model.png\\n\")\n","\n","    def train(self):\n","        \"\"\"Default training\"\"\"\n","\n","        self.f1_test_history = []\n","        self.f1_dev_history = []\n","\n","        for epoch in range(self.epochs):\n","            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n","            for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n","                labels, tokens, casing,char = batch\n","                self.model.train_on_batch([tokens, casing,char], labels)\n","\n","            # compute F1 scores\n","            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n","            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n","            self.f1_test_history.append(f1_test)\n","            print(\"f1 test \", round(f1_test, 4))\n","\n","            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n","            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n","            self.f1_dev_history.append(f1_dev)\n","            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n","\n","        print(\"Final F1 test score: \", f1_test)\n","\n","        print(\"Training finished.\")\n","\n","        # save model\n","        self.modelName = \"{}_{}_{}_{}_{}_{}_{}\".format(self.epochs,\n","                                                        self.dropout,\n","                                                        self.dropout_recurrent,\n","                                                        self.lstm_state_size,\n","                                                        self.conv_size,\n","                                                        self.learning_rate,\n","                                                        self.optimizer.__class__.__name__\n","                                                       )\n","\n","        modelName = self.modelName + \".h5\"\n","        self.model.save(modelName)\n","        print(\"Model weights saved.\")\n","\n","        self.model.set_weights(self.init_weights)  # clear model\n","        print(\"Model weights cleared.\")\n","\n","    def writeToFile(self):\n","        \"\"\"Write output to file\"\"\"\n","        # .txt file format\n","        # [epoch  ]\n","        # [f1_test]\n","        # [f1_dev ]\n","        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n","        fileName = self.modelName + \".txt\"\n","        with open(fileName,'wb') as f:\n","            for line in output:\n","                np.savetxt(f, line, fmt='%.5f')\n","        print(\"Model performance written to file\")\n","    print(\"Class initialised\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6nLXMIpPaEV","executionInfo":{"status":"ok","timestamp":1701576361554,"user_tz":360,"elapsed":176,"user":{"displayName":"Saikrishna Sanniboina","userId":"04267939723008232652"}},"outputId":"43b793ad-c78c-4624-beb7-cd43b17c16c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class initialised\n"]}]},{"cell_type":"code","source":["\n","\n","\"\"\"Set parameters\"\"\"\n","\n","EPOCHS = 10               # paper: 80\n","DROPOUT = 0.5             # paper: 0.68\n","DROPOUT_RECURRENT = 0.25  # not specified in paper, 0.25 recommended\n","LSTM_STATE_SIZE = 200     # paper: 275\n","CONV_SIZE = 3             # paper: 3\n","LEARNING_RATE = 0.0105    # paper 0.0105\n","OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended\n","\n"],"metadata":{"id":"0Ab63AV0QLHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Construct and run model\"\"\"\n","cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n","cnn_blstm.loadData()\n","cnn_blstm.addCharInfo()\n","cnn_blstm.embed()\n","cnn_blstm.createBatches()\n","cnn_blstm.buildModel()\n","cnn_blstm.train()\n","cnn_blstm.writeToFile()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XE0eAnfbQUwM","outputId":"dae0d581-e5ec-4224-e90f-d302eac38894"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model built. Saved model.png\n","\n","Epoch 0/10\n","f1 test  0.0663\n","f1 dev  0.0761 \n","\n","Epoch 1/10\n","f1 test  0.37\n","f1 dev  0.4099 \n","\n","Epoch 2/10\n","f1 test  0.5172\n","f1 dev  0.5359 \n","\n","Epoch 3/10\n","f1 test  0.5951\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bMgmZadDQX4q"},"execution_count":null,"outputs":[]}]}